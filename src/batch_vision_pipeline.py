"""
Batch Vision Pipeline - Cost-Effective Approach

This is the BETTER strategy for video processing:
1. Video → Extract frames with OpenCV
2. Each frame → Local VLM (BLIP-2) or simulated → Text description (FREE)
3. Collect ALL frame descriptions with metadata
4. ONE LLM call (Groq/Llama - FREE) → Complete security analysis

Cost Comparison:
- Direct GPT-4 Vision: ~$0.02/frame × 50 frames = $1.00 per video
- This approach: BLIP-2 (free) + 1 Groq call (free) = $0.00

Pipeline Flow:
+-------------+     +------------------+     +-------------------------+
| Video File  | --> | OpenCV Extract   | --> | Frame + Metadata        |
+-------------+     | Frames           |     +-------------------------+
                    +------------------+               |
                                                       v
                                             +-------------------------+
                                             | BLIP-2 / Local VLM      |
                                             | Generate text caption   |
                                             | for each frame (FREE)   |
                                             +-------------------------+
                                                       |
                                                       v
                                             +-------------------------+
                                             | Collect ALL frames:     |
                                             | - Frame 1: "Blue truck.."|
                                             | - Frame 2: "Person..."  |
                                             | - Frame N: "..."        |
                                             +-------------------------+
                                                       |
                                                       v
                                             +-------------------------+
                                             | ONE LLM Call (Groq)     |
                                             | Analyze all frames      |
                                             | Generate alerts         |
                                             | Create summary          |
                                             +-------------------------+
                                                       |
                                                       v
                                             +-------------------------+
                                             | Results:                |
                                             | - Per-frame analysis    |
                                             | - Security alerts       |
                                             | - Video summary         |
                                             +-------------------------+
"""

import os
import sys
import cv2
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Callable
from dataclasses import dataclass, field
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Check for PIL
try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

# Check for BLIP-2
BLIP_AVAILABLE = False
TORCH_AVAILABLE = False
CUDA_AVAILABLE = False
GPU_NAME = "N/A"
try:
    import torch
    TORCH_AVAILABLE = True
    CUDA_AVAILABLE = torch.cuda.is_available()
    if CUDA_AVAILABLE:
        GPU_NAME = torch.cuda.get_device_name(0)
        GPU_MEMORY = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
        logger.info(f"GPU detected: {GPU_NAME} with {GPU_MEMORY:.1f}GB VRAM")
    from transformers import Blip2Processor, Blip2ForConditionalGeneration
    BLIP_AVAILABLE = CUDA_AVAILABLE  # Only useful with GPU
except ImportError as e:
    logger.warning(f"BLIP-2 dependencies not available: {e}")

# Import Groq LLM
try:
    from langchain_groq import ChatGroq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False

# Import config
try:
    from .config import GROQ_API_KEY, GROQ_MODEL_NAME
except ImportError:
    GROQ_API_KEY = os.getenv("GROQ_API_KEY")
    GROQ_MODEL_NAME = "llama-3.3-70b-versatile"


@dataclass
class FrameData:
    """Data for a single extracted frame."""
    frame_id: int
    timestamp: datetime
    location: Dict
    telemetry: Dict
    description: str = ""  # Generated by VLM
    image_data: any = None  # Raw image (optional, for display)


@dataclass
class BatchAnalysisResult:
    """Complete analysis result for a video."""
    frames: List[Dict]  # Per-frame analysis
    alerts: List[Dict]  # All security alerts
    summary: str  # Video summary
    statistics: Dict  # Detection statistics
    threat_assessment: str  # Overall threat assessment
    processing_time_ms: float = 0


@dataclass
class BatchPipelineConfig:
    """Configuration for batch pipeline."""
    # Frame extraction
    frame_interval_seconds: int = 5
    max_frames: int = 50

    # VLM for captioning: "auto", "blip2", "simulated"
    # "auto" will use BLIP/BLIP-2 if GPU available, else simulated
    vlm_provider: str = "auto"

    # LLM for analysis
    llm_provider: str = "groq"  # "groq", "openai"

    # Location zones
    location_zones: List[Dict] = field(default_factory=lambda: [
        {"name": "Main Gate", "zone": "perimeter"},
        {"name": "Parking Lot", "zone": "parking"},
        {"name": "Warehouse", "zone": "storage"},
        {"name": "Loading Dock", "zone": "operations"},
        {"name": "Back Fence", "zone": "perimeter"},
    ])


class LocalVLMCaptioner:
    """
    Local VLM for generating frame descriptions.
    Uses BLIP-2 for actual captioning or simulated for demo.

    Model options based on GPU VRAM:
    - 4GB VRAM: Use BLIP (not BLIP-2) or CPU offloading
    - 8GB+ VRAM: Use BLIP-2 (2.7B)
    - 16GB+ VRAM: Use BLIP-2 (6.7B) for best quality
    """

    def __init__(self, provider: str = "simulated"):
        self.provider = provider
        self.model = None
        self.processor = None

        # Auto-detect best provider
        if provider == "auto":
            if BLIP_AVAILABLE:
                self.provider = "blip2"
            else:
                self.provider = "simulated"
            logger.info(f"Auto-selected provider: {self.provider}")

        if self.provider == "blip2" and BLIP_AVAILABLE:
            self._load_blip2()

    def _load_blip2(self):
        """Load BLIP-2 model optimized for available GPU memory."""
        try:
            # Check GPU memory
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            logger.info(f"GPU Memory: {gpu_memory:.1f}GB")

            if gpu_memory < 6:
                # For 4GB GPU (like GTX 1650), use smaller BLIP model or CPU offloading
                logger.info("Using BLIP (smaller model) for limited GPU memory...")
                from transformers import BlipProcessor, BlipForConditionalGeneration
                self.processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
                self.model = BlipForConditionalGeneration.from_pretrained(
                    "Salesforce/blip-image-captioning-base",
                    torch_dtype=torch.float16
                )
                self.model.to("cuda")
                self.model_type = "blip"
                logger.info("BLIP (base) loaded on GPU")
            else:
                # For 8GB+ GPU, use BLIP-2
                logger.info("Loading BLIP-2 model (this may take a moment)...")
                self.processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
                self.model = Blip2ForConditionalGeneration.from_pretrained(
                    "Salesforce/blip2-opt-2.7b",
                    torch_dtype=torch.float16,
                    device_map="auto"  # Automatic device placement
                )
                self.model_type = "blip2"
                logger.info("BLIP-2 loaded on GPU")
        except Exception as e:
            logger.error(f"Failed to load BLIP model: {e}")
            logger.info("Falling back to simulated mode")
            self.provider = "simulated"
            self.model = None

    def caption_frame(self, image_data, prompt: str = None) -> str:
        """Generate caption for a frame."""
        if self.provider == "blip2" and self.model:
            return self._blip_caption(image_data, prompt)
        else:
            return self._simulated_caption()

    def _blip_caption(self, image_data, prompt: str = None) -> str:
        """Generate caption using BLIP or BLIP-2."""
        if not PIL_AVAILABLE:
            return self._simulated_caption()

        try:
            # Convert to PIL if needed
            if not isinstance(image_data, Image.Image):
                image = Image.fromarray(image_data)
            else:
                image = image_data

            # Resize if too large (save GPU memory)
            max_size = 384
            if max(image.size) > max_size:
                image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)

            # Different prompting for BLIP vs BLIP-2
            if hasattr(self, 'model_type') and self.model_type == "blip":
                # BLIP (base) - simpler captioning
                inputs = self.processor(images=image, return_tensors="pt")
                inputs = {k: v.to("cuda") for k, v in inputs.items()}

                with torch.no_grad():
                    generated_ids = self.model.generate(**inputs, max_length=50)
            else:
                # BLIP-2 - can use prompts
                security_prompt = prompt or "Describe this security camera image, focusing on people, vehicles, and activities:"
                inputs = self.processor(images=image, text=security_prompt, return_tensors="pt")
                inputs = {k: v.to("cuda") if hasattr(v, 'to') else v for k, v in inputs.items()}

                with torch.no_grad():
                    generated_ids = self.model.generate(**inputs, max_length=100)

            caption = self.processor.decode(generated_ids[0], skip_special_tokens=True)
            return caption.strip()

        except Exception as e:
            logger.error(f"Error generating caption: {e}")
            return self._simulated_caption()

    def _simulated_caption(self) -> str:
        """Generate simulated caption for demo."""
        import random
        scenarios = [
            "Blue Ford F150 pickup truck entering through main gate",
            "Person in dark hoodie walking near perimeter fence",
            "White delivery van parked at loading dock",
            "Two workers in safety vests near warehouse entrance",
            "Empty parking lot with no visible activity",
            "Person carrying large bag near back fence",
            "Red sedan exiting through main gate",
            "Security guard on patrol near office building",
            "Unknown person loitering near restricted area",
            "Motorcycle parked near garage entrance",
            "Person with face partially covered near fence",
            "Delivery truck unloading packages at dock",
        ]
        return random.choice(scenarios)


class BatchVisionPipeline:
    """
    Cost-effective batch vision pipeline.

    1. Extracts frames from video
    2. Generates text descriptions using local VLM (FREE)
    3. Sends ALL descriptions to LLM in ONE call (FREE with Groq)
    4. Returns comprehensive analysis
    """

    BATCH_ANALYSIS_PROMPT = """You are an expert security analyst reviewing drone surveillance footage.

I have {num_frames} frames from a surveillance video. Analyze ALL frames together and provide a comprehensive security assessment.

SECURITY ALERT RULES:
- R001 Night Activity (HIGH): Person detected between 00:00-05:00
- R002 Loitering Detection (HIGH): Person in same area for extended period
- R003 Perimeter Activity (MEDIUM): Activity in perimeter zone
- R004 Repeat Vehicle (LOW): Same vehicle seen multiple times
- R005 Unknown Vehicle (MEDIUM): Unrecognized vehicle in restricted area
- R006 Suspicious Behavior (HIGH): Face covering, hiding, suspicious items

FRAME DATA:
{frame_data}

INSTRUCTIONS:
1. Analyze each frame and identify objects (people, vehicles, items)
2. Check security rules for EACH frame
3. Track patterns across frames (same person/vehicle appearing multiple times)
4. Generate alerts for any security concerns
5. Provide an overall summary and threat assessment

Respond in this EXACT JSON format:
{{
    "frame_analysis": [
        {{
            "frame_id": 1,
            "objects": [{{"type": "person/vehicle", "description": "details"}}],
            "alerts": [{{"rule_id": "R00X", "name": "Rule Name", "priority": "HIGH/MEDIUM/LOW", "reason": "why"}}],
            "threat_level": "NONE/LOW/MEDIUM/HIGH"
        }}
    ],
    "all_alerts": [
        {{"frame_id": 1, "rule_id": "R001", "name": "Night Activity", "priority": "HIGH", "reason": "Person detected at 02:30"}}
    ],
    "patterns_detected": ["Blue truck seen in frames 1, 5, 8 - possible surveillance", "Person loitering frames 3-7"],
    "summary": "Brief summary of surveillance period",
    "threat_assessment": "NONE/LOW/MEDIUM/HIGH/CRITICAL",
    "recommendations": ["Immediate action items if any"]
}}"""

    def __init__(self, config: BatchPipelineConfig = None):
        self.config = config or BatchPipelineConfig()

        # Initialize VLM captioner
        self.captioner = LocalVLMCaptioner(provider=self.config.vlm_provider)

        # Initialize LLM
        self.llm = None
        if GROQ_AVAILABLE and GROQ_API_KEY:
            self.llm = ChatGroq(
                model=GROQ_MODEL_NAME,
                temperature=0.1,
                api_key=GROQ_API_KEY
            )
            logger.info(f"Using Groq LLM: {GROQ_MODEL_NAME}")
        else:
            logger.warning("Groq LLM not available - will return raw descriptions only")

    def extract_frames(self, video_path: str) -> List[FrameData]:
        """Extract frames from video and generate captions."""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Cannot open video: {video_path}")

        fps = max(cap.get(cv2.CAP_PROP_FPS), 1)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps
        frame_skip = int(fps * self.config.frame_interval_seconds)

        logger.info(f"Video: {duration:.1f}s, extracting every {self.config.frame_interval_seconds}s")

        frames = []
        frame_count = 0
        extracted_count = 0
        start_time = datetime.now()

        while cap.isOpened() and extracted_count < self.config.max_frames:
            ret, frame = cap.read()
            if not ret:
                break

            if frame_count % max(frame_skip, 1) == 0:
                # Convert BGR to RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # Calculate timestamp
                timestamp = start_time + timedelta(seconds=frame_count / fps)

                # Get location
                location = self.config.location_zones[extracted_count % len(self.config.location_zones)]

                # Generate caption using VLM
                description = self.captioner.caption_frame(frame_rgb)

                # Create frame data
                frame_data = FrameData(
                    frame_id=extracted_count + 1,
                    timestamp=timestamp,
                    location=location,
                    telemetry={
                        "drone_id": "DRONE-001",
                        "altitude": 50,
                        "battery": max(100 - extracted_count * 2, 20)
                    },
                    description=description
                )

                frames.append(frame_data)
                extracted_count += 1

                logger.info(f"Frame {extracted_count}: {description[:50]}...")

            frame_count += 1

        cap.release()
        logger.info(f"Extracted {len(frames)} frames")
        return frames

    def analyze_batch(self, frames: List[FrameData]) -> BatchAnalysisResult:
        """
        Analyze all frames in ONE LLM call.

        This is the key cost-saving step - instead of calling Vision API
        for each frame, we send all text descriptions to LLM at once.
        """
        import time
        start_time = time.time()

        # Build frame data string for prompt
        frame_data_str = ""
        for f in frames:
            time_context = "NIGHT (restricted)" if 0 <= f.timestamp.hour < 5 else "Day"
            frame_data_str += f"""
Frame {f.frame_id}:
  - Time: {f.timestamp.strftime('%Y-%m-%d %H:%M:%S')} ({time_context})
  - Location: {f.location['name']} (Zone: {f.location['zone']})
  - Description: {f.description}
"""

        # If no LLM, return basic analysis
        if not self.llm:
            return self._basic_analysis(frames, time.time() - start_time)

        # Build and send prompt
        prompt = self.BATCH_ANALYSIS_PROMPT.format(
            num_frames=len(frames),
            frame_data=frame_data_str
        )

        try:
            response = self.llm.invoke(prompt)
            response_text = response.content if hasattr(response, 'content') else str(response)

            # Parse JSON from response
            import re
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                result = json.loads(json_match.group())

                return BatchAnalysisResult(
                    frames=result.get("frame_analysis", []),
                    alerts=result.get("all_alerts", []),
                    summary=result.get("summary", ""),
                    statistics={
                        "total_frames": len(frames),
                        "patterns": result.get("patterns_detected", [])
                    },
                    threat_assessment=result.get("threat_assessment", "UNKNOWN"),
                    processing_time_ms=(time.time() - start_time) * 1000
                )
        except Exception as e:
            logger.error(f"LLM analysis failed: {e}")

        return self._basic_analysis(frames, time.time() - start_time)

    def _basic_analysis(self, frames: List[FrameData], elapsed: float) -> BatchAnalysisResult:
        """Fallback basic analysis without LLM."""
        frame_results = []
        all_alerts = []

        for f in frames:
            # Basic keyword detection
            desc_lower = f.description.lower()
            objects = []
            alerts = []

            # Detect objects
            if any(w in desc_lower for w in ["person", "man", "woman", "worker", "guard"]):
                objects.append({"type": "person", "description": f.description})

                # Check night activity
                if 0 <= f.timestamp.hour < 5:
                    alert = {
                        "frame_id": f.frame_id,
                        "rule_id": "R001",
                        "name": "Night Activity",
                        "priority": "HIGH",
                        "reason": f"Person detected at {f.timestamp.strftime('%H:%M')}"
                    }
                    alerts.append(alert)
                    all_alerts.append(alert)

                # Check perimeter
                if f.location["zone"] == "perimeter":
                    alert = {
                        "frame_id": f.frame_id,
                        "rule_id": "R003",
                        "name": "Perimeter Activity",
                        "priority": "MEDIUM",
                        "reason": f"Person in perimeter zone at {f.location['name']}"
                    }
                    alerts.append(alert)
                    all_alerts.append(alert)

            if any(w in desc_lower for w in ["truck", "car", "van", "vehicle", "sedan"]):
                objects.append({"type": "vehicle", "description": f.description})

            # Check suspicious behavior
            if any(w in desc_lower for w in ["covered", "hiding", "suspicious", "loitering", "unknown"]):
                alert = {
                    "frame_id": f.frame_id,
                    "rule_id": "R006",
                    "name": "Suspicious Behavior",
                    "priority": "HIGH",
                    "reason": f.description[:50]
                }
                alerts.append(alert)
                all_alerts.append(alert)

            threat = "NONE"
            if any(a["priority"] == "HIGH" for a in alerts):
                threat = "HIGH"
            elif any(a["priority"] == "MEDIUM" for a in alerts):
                threat = "MEDIUM"
            elif alerts:
                threat = "LOW"

            frame_results.append({
                "frame_id": f.frame_id,
                "timestamp": f.timestamp.isoformat(),
                "location": f.location,
                "description": f.description,
                "objects": objects,
                "alerts": alerts,
                "threat_level": threat
            })

        # Overall threat
        high_alerts = sum(1 for a in all_alerts if a["priority"] == "HIGH")
        overall_threat = "CRITICAL" if high_alerts > 3 else "HIGH" if high_alerts > 0 else "MEDIUM" if all_alerts else "LOW"

        return BatchAnalysisResult(
            frames=frame_results,
            alerts=all_alerts,
            summary=f"Processed {len(frames)} frames. Found {len(all_alerts)} security alerts.",
            statistics={
                "total_frames": len(frames),
                "total_alerts": len(all_alerts),
                "high_priority": high_alerts
            },
            threat_assessment=overall_threat,
            processing_time_ms=elapsed * 1000
        )

    def process_video(self, video_path: str, progress_callback: Callable = None) -> BatchAnalysisResult:
        """
        Process entire video - the main entry point.

        1. Extract frames and generate captions (FREE with local VLM)
        2. Analyze all frames in ONE LLM call (FREE with Groq)
        """
        logger.info(f"Processing video: {video_path}")

        # Step 1: Extract frames and caption
        frames = self.extract_frames(video_path)

        if progress_callback:
            progress_callback(len(frames), len(frames), "Frames extracted")

        # Step 2: Batch analyze with LLM
        logger.info("Sending all frames to LLM for analysis...")
        result = self.analyze_batch(frames)

        logger.info(f"Analysis complete: {len(result.alerts)} alerts, threat level: {result.threat_assessment}")

        return result


def process_video_batch(
    video_path: str,
    frame_interval: int = 5,
    max_frames: int = 50,
    vlm_provider: str = "simulated"
) -> Dict:
    """
    Convenience function to process video with batch pipeline.

    Cost: FREE (BLIP-2 local + Groq API)

    Args:
        video_path: Path to video file
        frame_interval: Seconds between frames
        max_frames: Maximum frames to extract
        vlm_provider: "blip2" or "simulated"

    Returns:
        Complete analysis result
    """
    config = BatchPipelineConfig(
        frame_interval_seconds=frame_interval,
        max_frames=max_frames,
        vlm_provider=vlm_provider
    )

    pipeline = BatchVisionPipeline(config=config)
    result = pipeline.process_video(video_path)

    return {
        "frames": result.frames,
        "alerts": result.alerts,
        "summary": result.summary,
        "statistics": result.statistics,
        "threat_assessment": result.threat_assessment,
        "processing_time_ms": result.processing_time_ms
    }


def get_batch_pipeline_status() -> Dict:
    """Get status of batch pipeline components."""
    gpu_info = "N/A"
    gpu_memory = 0
    if CUDA_AVAILABLE:
        try:
            gpu_info = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        except:
            pass

    return {
        "blip2_available": BLIP_AVAILABLE,
        "cuda_available": CUDA_AVAILABLE,
        "gpu_name": gpu_info,
        "gpu_memory_gb": round(gpu_memory, 1),
        "groq_available": GROQ_AVAILABLE and bool(GROQ_API_KEY),
        "recommended_vlm": "blip2" if BLIP_AVAILABLE else "simulated",
        "cost": "FREE (local VLM + Groq)",
        "api_calls_per_video": 1  # Just ONE LLM call!
    }


if __name__ == "__main__":
    print("=" * 60)
    print("BATCH VISION PIPELINE - Cost Effective Approach")
    print("=" * 60)

    status = get_batch_pipeline_status()
    print(f"\nStatus: {json.dumps(status, indent=2)}")

    print("\nCost Comparison:")
    print("  - GPT-4 Vision per frame: ~$0.02 x 50 frames = $1.00/video")
    print("  - This approach: BLIP-2 (free) + 1 Groq call (free) = $0.00")

    # Test with simulated data
    print("\n--- Testing Batch Analysis ---")

    config = BatchPipelineConfig(vlm_provider="simulated")
    pipeline = BatchVisionPipeline(config=config)

    # Create simulated frames
    frames = []
    for i in range(5):
        frames.append(FrameData(
            frame_id=i + 1,
            timestamp=datetime.now() + timedelta(seconds=i * 5),
            location=config.location_zones[i % len(config.location_zones)],
            telemetry={"drone_id": "DRONE-001"},
            description=pipeline.captioner.caption_frame(None)
        ))

    # Analyze batch
    result = pipeline.analyze_batch(frames)

    print(f"\nResults:")
    print(f"  Frames analyzed: {len(result.frames)}")
    print(f"  Alerts: {len(result.alerts)}")
    print(f"  Threat Assessment: {result.threat_assessment}")
    print(f"  Processing Time: {result.processing_time_ms:.0f}ms")
    print(f"\nSummary: {result.summary}")
